{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! python -m venv .venv\n",
    "# ! python.exe -m pip install -U --upgrade-strategy eager --no-cache-dir pip\n",
    "# ! pip install albumentations opencv-python numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import albumentations as A\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def augment_dataset(input_dir, output_dir):\n",
    "    print(f\"1. Input directory: {input_dir}\")\n",
    "    print(f\"2. Output directory: {output_dir}\")\n",
    "\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "        print(f\"3. Created output directory: {output_dir}\")\n",
    "    else:\n",
    "        print(f\"3. Output directory already exists: {output_dir}\")\n",
    "\n",
    "    # Define individual transformations\n",
    "    transformations = [\n",
    "        (\n",
    "            \"brightness_contrast\",\n",
    "            A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2),\n",
    "        ),\n",
    "        (\n",
    "            \"hue_saturation\",\n",
    "            A.HueSaturationValue(\n",
    "                hue_shift_limit=20, sat_shift_limit=30, val_shift_limit=20\n",
    "            ),\n",
    "        ),\n",
    "        (\"gaussian_noise\", A.GaussNoise(var_limit=(10.0, 50.0))),\n",
    "        (\"gamma\", A.RandomGamma(gamma_limit=(80, 120))),\n",
    "        (\"clahe\", A.CLAHE(clip_limit=4.0, tile_grid_size=(8, 8))),\n",
    "        (\"blur\", A.Blur(blur_limit=(3, 7))),\n",
    "        (\"gaussian_blur\", A.GaussianBlur(blur_limit=(3, 7))),\n",
    "        (\"motion_blur\", A.MotionBlur(blur_limit=(3, 7))),\n",
    "        (\"median_blur\", A.MedianBlur(blur_limit=3)),\n",
    "        (\"sharpen\", A.Sharpen(alpha=(0.2, 0.5), lightness=(0.5, 1.0))),\n",
    "        (\"emboss\", A.Emboss(alpha=(0.2, 0.5), strength=(0.2, 0.7))),\n",
    "        (\n",
    "            \"shadow\",\n",
    "            A.RandomShadow(\n",
    "                num_shadows_lower=1,\n",
    "                num_shadows_upper=2,\n",
    "                shadow_dimension=5,\n",
    "                shadow_roi=(0, 0.5, 1, 1),\n",
    "            ),\n",
    "        ),\n",
    "        (\"fog\", A.RandomFog(fog_coef_lower=0.1, fog_coef_upper=0.3, alpha_coef=0.08)),\n",
    "        (\n",
    "            \"sun_flare\",\n",
    "            A.RandomSunFlare(\n",
    "                flare_roi=(0, 0, 1, 0.5),\n",
    "                angle_lower=0,\n",
    "                angle_upper=1,\n",
    "                num_flare_circles_lower=6,\n",
    "                num_flare_circles_upper=10,\n",
    "                src_radius=400,\n",
    "                src_color=(255, 255, 255),\n",
    "            ),\n",
    "        ),\n",
    "        (\"tone_curve\", A.RandomToneCurve(scale=0.1)),\n",
    "        (\n",
    "            \"color_jitter\",\n",
    "            A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
    "        ),\n",
    "        (\"sepia\", A.ToSepia()),\n",
    "        (\"channel_shuffle\", A.ChannelShuffle()),\n",
    "        (\"rgb_shift\", A.RGBShift(r_shift_limit=20, g_shift_limit=20, b_shift_limit=20)),\n",
    "        (\"posterize\", A.Posterize(num_bits=4)),\n",
    "        (\"equalize\", A.Equalize()),\n",
    "        (\n",
    "            \"multiplicative_noise\",\n",
    "            A.MultiplicativeNoise(multiplier=(0.9, 1.1), per_channel=True),\n",
    "        ),\n",
    "        (\"iso_noise\", A.ISONoise(color_shift=(0.01, 0.05), intensity=(0.1, 0.5))),\n",
    "    ]\n",
    "\n",
    "    files = [\n",
    "        f\n",
    "        for f in os.listdir(input_dir)\n",
    "        if f.lower().endswith((\".jpg\", \".jpeg\", \".png\"))\n",
    "    ]\n",
    "    print(f\"4. Total number of image files in input directory: {len(files)}\")\n",
    "\n",
    "    images_processed = 0\n",
    "    for filename in files:\n",
    "        image_path = os.path.join(input_dir, filename)\n",
    "        print(f\"5. Processing image: {filename}\")\n",
    "\n",
    "        image = cv2.imread(image_path)\n",
    "        if image is None:\n",
    "            print(f\"Error: Failed to read image: {image_path}\")\n",
    "            continue\n",
    "\n",
    "        # Read corresponding annotation file\n",
    "        base_name = os.path.splitext(filename)[0]\n",
    "        ann_path = os.path.join(input_dir, f\"{base_name}.txt\")\n",
    "        if os.path.exists(ann_path):\n",
    "            with open(ann_path, \"r\") as f:\n",
    "                annotations = f.read()\n",
    "            print(f\"   Found annotation file: {ann_path}\")\n",
    "        else:\n",
    "            print(f\"   Warning: No annotation file found for {filename}\")\n",
    "            annotations = \"\"\n",
    "\n",
    "        # Apply each transformation\n",
    "        for transform_name, transform in transformations:\n",
    "            augmented = transform(image=image)\n",
    "            aug_image = augmented[\"image\"]\n",
    "\n",
    "            # Save augmented image\n",
    "            aug_filename = f\"{base_name}_{transform_name}.jpg\"\n",
    "            aug_path = os.path.join(output_dir, aug_filename)\n",
    "            cv2.imwrite(aug_path, aug_image)\n",
    "\n",
    "            # Copy original annotation file for augmented image\n",
    "            if annotations:\n",
    "                aug_ann_path = os.path.join(\n",
    "                    output_dir, f\"{base_name}_{transform_name}.txt\"\n",
    "                )\n",
    "                with open(aug_ann_path, \"w\") as f:\n",
    "                    f.write(annotations)\n",
    "\n",
    "        # Also save the original image and annotation to the output directory\n",
    "        cv2.imwrite(os.path.join(output_dir, filename), image)\n",
    "        if annotations:\n",
    "            with open(os.path.join(output_dir, f\"{base_name}.txt\"), \"w\") as f:\n",
    "                f.write(annotations)\n",
    "\n",
    "        images_processed += 1\n",
    "        print(f\"   Created {len(transformations)} augmented versions of {filename}\")\n",
    "\n",
    "    print(f\"\\nAugmentation completed!\")\n",
    "    print(f\"Total images processed: {images_processed}\")\n",
    "    print(f\"Total augmented images created: {images_processed * len(transformations)}\")\n",
    "    print(\n",
    "        f\"Total images in output (including originals): {images_processed * (len(transformations) + 1)}\"\n",
    "    )\n",
    "\n",
    "    # List files in output directory\n",
    "    output_files = os.listdir(output_dir)\n",
    "    print(f\"Total files in output directory: {len(output_files)}\")\n",
    "    print(f\"First 5 files in output directory: {output_files[:5]}\")\n",
    "\n",
    "\n",
    "# Run the augmentation function\n",
    "input_dir = \"data/Original_Data\"\n",
    "output_dir = \"data/Augmented_Data\"\n",
    "augment_dataset(input_dir, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset split complete. 30 groups in training, 10 groups in testing.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import re\n",
    "\n",
    "\n",
    "def split_dataset(source_dir, modelling_dir, train_ratio=0.75):\n",
    "    # Create train and test directories if they don't exist\n",
    "    for split in [\"train\", \"test\"]:\n",
    "        os.makedirs(os.path.join(modelling_dir, \"images\", split), exist_ok=True)\n",
    "        os.makedirs(os.path.join(modelling_dir, \"labels\", split), exist_ok=True)\n",
    "\n",
    "    # Get all files in the source directory\n",
    "    all_files = os.listdir(source_dir)\n",
    "\n",
    "    # Group files by their base name (without augmentation suffix and extension)\n",
    "    file_groups = {}\n",
    "    for file in all_files:\n",
    "        # Use regex to extract the base name\n",
    "        match = re.match(r\"(DJI_\\d+_\\d+_D).*\", file)\n",
    "        if match:\n",
    "            base_name = match.group(1)\n",
    "            if base_name not in file_groups:\n",
    "                file_groups[base_name] = []\n",
    "            file_groups[base_name].append(file)\n",
    "\n",
    "    # Randomly split the groups\n",
    "    all_groups = list(file_groups.keys())\n",
    "    random.shuffle(all_groups)\n",
    "    split_index = int(len(all_groups) * train_ratio)\n",
    "    train_groups = all_groups[:split_index]\n",
    "    test_groups = all_groups[split_index:]\n",
    "\n",
    "    # Move files to their respective directories\n",
    "    for groups, split in [(train_groups, \"train\"), (test_groups, \"test\")]:\n",
    "        for group in groups:\n",
    "            for file in file_groups[group]:\n",
    "                source_path = os.path.join(source_dir, file)\n",
    "                if file.lower().endswith((\".jpg\", \".jpeg\", \".png\", \".tif\", \".tiff\")):\n",
    "                    dest_path = os.path.join(modelling_dir, \"images\", split, file)\n",
    "                elif file.lower().endswith(\".txt\"):\n",
    "                    dest_path = os.path.join(modelling_dir, \"labels\", split, file)\n",
    "                else:\n",
    "                    continue  # Skip files that are neither images nor labels\n",
    "                shutil.move(source_path, dest_path)\n",
    "\n",
    "    print(\n",
    "        f\"Dataset split complete. {len(train_groups)} groups in training, {len(test_groups)} groups in testing.\"\n",
    "    )\n",
    "\n",
    "\n",
    "# Usage\n",
    "source_directory = \"data/Augmented Data\"\n",
    "modelling_directory = \"data/Modelling\"\n",
    "\n",
    "split_dataset(source_directory, modelling_directory)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
