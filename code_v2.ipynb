{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!pip install -r requirements.txt\n","# !conda install -c conda-forge libgl\n","# !pip list\n","\n","# ! sudo apt install libgl1-mesa-glx"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["      1/200         0G       6.21      6.142      5.681        368        640:  83%|████████▎ | 5/6 [02:53<00:34, 34.61s/it]\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[1;32m/home/sagemaker-user/the-plant-project-SU-dataset/code_v2.ipynb Cell 2\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell://bflckg8wa85jc1q.studio.us-west-2.sagemaker.aws/home/sagemaker-user/the-plant-project-SU-dataset/code_v2.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=163'>164</a>\u001b[0m process_val_images(model)\n\u001b[1;32m    <a href='vscode-notebook-cell://bflckg8wa85jc1q.studio.us-west-2.sagemaker.aws/home/sagemaker-user/the-plant-project-SU-dataset/code_v2.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=165'>166</a>\u001b[0m \u001b[39m# Train the model with custom settings\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell://bflckg8wa85jc1q.studio.us-west-2.sagemaker.aws/home/sagemaker-user/the-plant-project-SU-dataset/code_v2.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=166'>167</a>\u001b[0m results \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mtrain(\n\u001b[1;32m    <a href='vscode-notebook-cell://bflckg8wa85jc1q.studio.us-west-2.sagemaker.aws/home/sagemaker-user/the-plant-project-SU-dataset/code_v2.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=167'>168</a>\u001b[0m     data\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mconfig.yaml\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    <a href='vscode-notebook-cell://bflckg8wa85jc1q.studio.us-west-2.sagemaker.aws/home/sagemaker-user/the-plant-project-SU-dataset/code_v2.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=168'>169</a>\u001b[0m     epochs\u001b[39m=\u001b[39;49m\u001b[39m200\u001b[39;49m,\n\u001b[1;32m    <a href='vscode-notebook-cell://bflckg8wa85jc1q.studio.us-west-2.sagemaker.aws/home/sagemaker-user/the-plant-project-SU-dataset/code_v2.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=169'>170</a>\u001b[0m     imgsz\u001b[39m=\u001b[39;49m\u001b[39m640\u001b[39;49m,\n\u001b[1;32m    <a href='vscode-notebook-cell://bflckg8wa85jc1q.studio.us-west-2.sagemaker.aws/home/sagemaker-user/the-plant-project-SU-dataset/code_v2.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=170'>171</a>\u001b[0m     batch\u001b[39m=\u001b[39;49m\u001b[39m4\u001b[39;49m,\n\u001b[1;32m    <a href='vscode-notebook-cell://bflckg8wa85jc1q.studio.us-west-2.sagemaker.aws/home/sagemaker-user/the-plant-project-SU-dataset/code_v2.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=171'>172</a>\u001b[0m     optimizer\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mAdamW\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    <a href='vscode-notebook-cell://bflckg8wa85jc1q.studio.us-west-2.sagemaker.aws/home/sagemaker-user/the-plant-project-SU-dataset/code_v2.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=172'>173</a>\u001b[0m     lr0\u001b[39m=\u001b[39;49m\u001b[39m1e-3\u001b[39;49m,\n\u001b[1;32m    <a href='vscode-notebook-cell://bflckg8wa85jc1q.studio.us-west-2.sagemaker.aws/home/sagemaker-user/the-plant-project-SU-dataset/code_v2.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=173'>174</a>\u001b[0m     lrf\u001b[39m=\u001b[39;49m\u001b[39m1e-4\u001b[39;49m,\n\u001b[1;32m    <a href='vscode-notebook-cell://bflckg8wa85jc1q.studio.us-west-2.sagemaker.aws/home/sagemaker-user/the-plant-project-SU-dataset/code_v2.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=174'>175</a>\u001b[0m     momentum\u001b[39m=\u001b[39;49m\u001b[39m0.937\u001b[39;49m,\n\u001b[1;32m    <a href='vscode-notebook-cell://bflckg8wa85jc1q.studio.us-west-2.sagemaker.aws/home/sagemaker-user/the-plant-project-SU-dataset/code_v2.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=175'>176</a>\u001b[0m     weight_decay\u001b[39m=\u001b[39;49m\u001b[39m0.0005\u001b[39;49m,\n\u001b[1;32m    <a href='vscode-notebook-cell://bflckg8wa85jc1q.studio.us-west-2.sagemaker.aws/home/sagemaker-user/the-plant-project-SU-dataset/code_v2.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=176'>177</a>\u001b[0m     warmup_epochs\u001b[39m=\u001b[39;49m\u001b[39m3\u001b[39;49m,\n\u001b[1;32m    <a href='vscode-notebook-cell://bflckg8wa85jc1q.studio.us-west-2.sagemaker.aws/home/sagemaker-user/the-plant-project-SU-dataset/code_v2.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=177'>178</a>\u001b[0m     warmup_momentum\u001b[39m=\u001b[39;49m\u001b[39m0.8\u001b[39;49m,\n\u001b[1;32m    <a href='vscode-notebook-cell://bflckg8wa85jc1q.studio.us-west-2.sagemaker.aws/home/sagemaker-user/the-plant-project-SU-dataset/code_v2.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=178'>179</a>\u001b[0m     warmup_bias_lr\u001b[39m=\u001b[39;49m\u001b[39m0.1\u001b[39;49m,\n\u001b[1;32m    <a href='vscode-notebook-cell://bflckg8wa85jc1q.studio.us-west-2.sagemaker.aws/home/sagemaker-user/the-plant-project-SU-dataset/code_v2.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=179'>180</a>\u001b[0m     box\u001b[39m=\u001b[39;49m\u001b[39m7.5\u001b[39;49m,\n\u001b[1;32m    <a href='vscode-notebook-cell://bflckg8wa85jc1q.studio.us-west-2.sagemaker.aws/home/sagemaker-user/the-plant-project-SU-dataset/code_v2.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=180'>181</a>\u001b[0m     \u001b[39mcls\u001b[39;49m\u001b[39m=\u001b[39;49m\u001b[39m0.5\u001b[39;49m,\n\u001b[1;32m    <a href='vscode-notebook-cell://bflckg8wa85jc1q.studio.us-west-2.sagemaker.aws/home/sagemaker-user/the-plant-project-SU-dataset/code_v2.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=181'>182</a>\u001b[0m     dfl\u001b[39m=\u001b[39;49m\u001b[39m1.5\u001b[39;49m,\n\u001b[1;32m    <a href='vscode-notebook-cell://bflckg8wa85jc1q.studio.us-west-2.sagemaker.aws/home/sagemaker-user/the-plant-project-SU-dataset/code_v2.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=182'>183</a>\u001b[0m     mixup\u001b[39m=\u001b[39;49m\u001b[39m0.2\u001b[39;49m,\n\u001b[1;32m    <a href='vscode-notebook-cell://bflckg8wa85jc1q.studio.us-west-2.sagemaker.aws/home/sagemaker-user/the-plant-project-SU-dataset/code_v2.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=183'>184</a>\u001b[0m     copy_paste\u001b[39m=\u001b[39;49m\u001b[39m0.1\u001b[39;49m,\n\u001b[1;32m    <a href='vscode-notebook-cell://bflckg8wa85jc1q.studio.us-west-2.sagemaker.aws/home/sagemaker-user/the-plant-project-SU-dataset/code_v2.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=184'>185</a>\u001b[0m     device\u001b[39m=\u001b[39;49mdevice,\n\u001b[1;32m    <a href='vscode-notebook-cell://bflckg8wa85jc1q.studio.us-west-2.sagemaker.aws/home/sagemaker-user/the-plant-project-SU-dataset/code_v2.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=185'>186</a>\u001b[0m     amp\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,  \u001b[39m# Enable mixed precision training\u001b[39;49;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://bflckg8wa85jc1q.studio.us-west-2.sagemaker.aws/home/sagemaker-user/the-plant-project-SU-dataset/code_v2.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=186'>187</a>\u001b[0m     workers\u001b[39m=\u001b[39;49m\u001b[39m4\u001b[39;49m,  \u001b[39m# Number of worker threads for data loading\u001b[39;49;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://bflckg8wa85jc1q.studio.us-west-2.sagemaker.aws/home/sagemaker-user/the-plant-project-SU-dataset/code_v2.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=187'>188</a>\u001b[0m     cos_lr\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,  \u001b[39m# Use cosine learning rate scheduler\u001b[39;49;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://bflckg8wa85jc1q.studio.us-west-2.sagemaker.aws/home/sagemaker-user/the-plant-project-SU-dataset/code_v2.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=188'>189</a>\u001b[0m     \u001b[39m# resume=True,  # Resume training from last checkpoint\u001b[39;49;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://bflckg8wa85jc1q.studio.us-west-2.sagemaker.aws/home/sagemaker-user/the-plant-project-SU-dataset/code_v2.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=189'>190</a>\u001b[0m     patience\u001b[39m=\u001b[39;49m\u001b[39m20\u001b[39;49m,  \u001b[39m# Early stopping\u001b[39;49;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://bflckg8wa85jc1q.studio.us-west-2.sagemaker.aws/home/sagemaker-user/the-plant-project-SU-dataset/code_v2.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=190'>191</a>\u001b[0m     val\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    <a href='vscode-notebook-cell://bflckg8wa85jc1q.studio.us-west-2.sagemaker.aws/home/sagemaker-user/the-plant-project-SU-dataset/code_v2.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=191'>192</a>\u001b[0m     save_period\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m,  \u001b[39m# Save model every 5 epochs (replaces val_period)\u001b[39;49;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://bflckg8wa85jc1q.studio.us-west-2.sagemaker.aws/home/sagemaker-user/the-plant-project-SU-dataset/code_v2.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=192'>193</a>\u001b[0m )\n\u001b[1;32m    <a href='vscode-notebook-cell://bflckg8wa85jc1q.studio.us-west-2.sagemaker.aws/home/sagemaker-user/the-plant-project-SU-dataset/code_v2.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=193'>194</a>\u001b[0m \u001b[39m# Evaluate the model\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://bflckg8wa85jc1q.studio.us-west-2.sagemaker.aws/home/sagemaker-user/the-plant-project-SU-dataset/code_v2.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=194'>195</a>\u001b[0m val_results \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mval()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/ultralytics/engine/model.py:810\u001b[0m, in \u001b[0;36mModel.train\u001b[0;34m(self, trainer, **kwargs)\u001b[0m\n\u001b[1;32m    807\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mmodel\n\u001b[1;32m    809\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mhub_session \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msession  \u001b[39m# attach optional HUB session\u001b[39;00m\n\u001b[0;32m--> 810\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrainer\u001b[39m.\u001b[39;49mtrain()\n\u001b[1;32m    811\u001b[0m \u001b[39m# Update model and cfg after training\u001b[39;00m\n\u001b[1;32m    812\u001b[0m \u001b[39mif\u001b[39;00m RANK \u001b[39min\u001b[39;00m {\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m}:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/ultralytics/engine/trainer.py:206\u001b[0m, in \u001b[0;36mBaseTrainer.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    203\u001b[0m         ddp_cleanup(\u001b[39mself\u001b[39m, \u001b[39mstr\u001b[39m(file))\n\u001b[1;32m    205\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 206\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_do_train(world_size)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/ultralytics/engine/trainer.py:395\u001b[0m, in \u001b[0;36mBaseTrainer._do_train\u001b[0;34m(self, world_size)\u001b[0m\n\u001b[1;32m    390\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtloss \u001b[39m=\u001b[39m (\n\u001b[1;32m    391\u001b[0m         (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtloss \u001b[39m*\u001b[39m i \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloss_items) \u001b[39m/\u001b[39m (i \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtloss \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloss_items\n\u001b[1;32m    392\u001b[0m     )\n\u001b[1;32m    394\u001b[0m \u001b[39m# Backward\u001b[39;00m\n\u001b[0;32m--> 395\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscaler\u001b[39m.\u001b[39;49mscale(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mloss)\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m    397\u001b[0m \u001b[39m# Optimize - https://pytorch.org/docs/master/notes/amp_examples.html\u001b[39;00m\n\u001b[1;32m    398\u001b[0m \u001b[39mif\u001b[39;00m ni \u001b[39m-\u001b[39m last_opt_step \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maccumulate:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    512\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    513\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    514\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    519\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    520\u001b[0m     )\n\u001b[0;32m--> 521\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    522\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    523\u001b[0m )\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    284\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    286\u001b[0m \u001b[39m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m _engine_run_backward(\n\u001b[1;32m    290\u001b[0m     tensors,\n\u001b[1;32m    291\u001b[0m     grad_tensors_,\n\u001b[1;32m    292\u001b[0m     retain_graph,\n\u001b[1;32m    293\u001b[0m     create_graph,\n\u001b[1;32m    294\u001b[0m     inputs,\n\u001b[1;32m    295\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    296\u001b[0m     accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    297\u001b[0m )\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/graph.py:768\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    766\u001b[0m     unregister_hooks \u001b[39m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    767\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 768\u001b[0m     \u001b[39mreturn\u001b[39;00m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    769\u001b[0m         t_outputs, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    770\u001b[0m     )  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    771\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    772\u001b[0m     \u001b[39mif\u001b[39;00m attach_logging_hooks:\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["# version 2\n","from ultralytics import YOLO\n","import os\n","import torch\n","from torch.utils.data import DataLoader\n","from torchvision import transforms\n","from PIL import Image\n","import yaml\n","import cv2\n","import numpy as np\n","\n","\n","# Add this import at the beginning of the file\n","def load_existing_labels(label_path):\n","    with open(label_path, \"r\") as f:\n","        return [line.strip().split() for line in f.readlines()]\n","\n","\n","def convert_yolo_to_bbox(yolo_bbox, img_width, img_height):\n","    x_center, y_center, width, height = map(float, yolo_bbox)\n","    x1 = int((x_center - width / 2) * img_width)\n","    y1 = int((y_center - height / 2) * img_height)\n","    x2 = int((x_center + width / 2) * img_width)\n","    y2 = int((y_center + height / 2) * img_height)\n","    return [x1, y1, x2, y2]\n","\n","\n","def convert_bbox_to_yolo(bbox, img_width, img_height):\n","    x1, y1, x2, y2 = bbox\n","    width = (x2 - x1) / img_width\n","    height = (y2 - y1) / img_height\n","    x_center = (x1 + x2) / (2 * img_width)\n","    y_center = (y1 + y2) / (2 * img_height)\n","    return [x_center, y_center, width, height]\n","\n","\n","def iou(box1, box2):\n","    x1 = max(box1[0], box2[0])\n","    y1 = max(box1[1], box2[1])\n","    x2 = min(box1[2], box2[2])\n","    y2 = min(box1[3], box2[3])\n","    intersection = max(0, x2 - x1) * max(0, y2 - y1)\n","    area1 = (box1[2] - box1[0]) * (box1[3] - box1[1])\n","    area2 = (box2[2] - box2[0]) * (box2[3] - box2[1])\n","    return intersection / (area1 + area2 - intersection)\n","\n","\n","def process_image(image_path, label_path, model, iou_threshold=0.5):\n","    print(f\"Processing image: {image_path}\")\n","    # Load image\n","    img = cv2.imread(image_path)\n","    img_height, img_width = img.shape[:2]\n","\n","    # Create label file if it doesn't exist\n","    if not os.path.exists(label_path):\n","        open(label_path, \"w\").close()\n","\n","    existing_labels = load_existing_labels(label_path)\n","\n","    existing_bboxes = [\n","        convert_yolo_to_bbox(label[1:], img_width, img_height)\n","        for label in existing_labels\n","    ]\n","    # Detect plants using YOLO\n","    results = model(img)\n","    new_labels = []\n","    # Check if there are any detections\n","    if len(results) > 0 and len(results[0].boxes) > 0:\n","        for det in results[0].boxes.data:\n","            x1, y1, x2, y2, conf, cls = det.tolist()\n","            if cls == 0:  # Assuming 0 is the class for plants\n","                new_bbox = [int(x1), int(y1), int(x2), int(y2)]\n","                # Check if the new detection overlaps with existing labels\n","                is_new = all(\n","                    iou(new_bbox, existing_bbox) < iou_threshold\n","                    for existing_bbox in existing_bboxes\n","                )\n","                if is_new:\n","                    yolo_bbox = convert_bbox_to_yolo(new_bbox, img_width, img_height)\n","                    new_labels.append([0] + yolo_bbox)\n","                    existing_bboxes.append(\n","                        new_bbox\n","                    )  # Add the new bbox to existing_bboxes\n","    return new_labels\n","\n","\n","def process_val_images(model):\n","    val_images_path = \"data/images/val\"\n","    val_labels_path = \"data/labels/val\"\n","    os.makedirs(val_labels_path, exist_ok=True)\n","\n","    for image_file in os.listdir(val_images_path):\n","        if image_file.lower().endswith((\".png\", \".jpg\", \".jpeg\")):\n","            image_path = os.path.join(val_images_path, image_file)\n","            label_file = os.path.splitext(image_file)[0] + \".txt\"\n","            label_path = os.path.join(val_labels_path, label_file)\n","            new_labels = process_image(image_path, label_path, model)\n","            with open(label_path, \"a\") as f:\n","                for label in new_labels:\n","                    f.write(\" \".join(map(str, label)) + \"\\n\")\n","            print(\n","                f\"Processed {image_file}: {len(new_labels)} new plants detected and labeled\"\n","            )\n","\n","\n","# Check for GPU availability and set device\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Using device: {device}\")\n","# Load a pretrained YOLOv8s model (smaller model for memory efficiency)\n","model = YOLO(\"yolov10x.pt\").to(device)\n","\n","\n","# Define data augmentation\n","transform = transforms.Compose(\n","    [\n","        transforms.RandomRotation(20),\n","        transforms.RandomHorizontalFlip(),\n","        transforms.RandomVerticalFlip(),\n","        transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n","        transforms.RandomResizedCrop(640, scale=(0.8, 1.0)),\n","    ]\n",")\n","# Custom dataset class for plant images\n","\n","\n","class PlantDataset(torch.utils.data.Dataset):\n","    def __init__(self, img_dir, label_dir, transform=None):\n","        self.img_dir = img_dir\n","        self.label_dir = label_dir\n","        self.transform = transform\n","        self.images = [\n","            img\n","            for img in os.listdir(img_dir)\n","            if img.endswith((\".png\", \".jpg\", \".jpeg\"))\n","        ]\n","\n","    def __len__(self):\n","        return len(self.images)\n","\n","    def __getitem__(self, idx):\n","        img_path = os.path.join(self.img_dir, self.images[idx])\n","        label_path = os.path.join(\n","            self.label_dir, self.images[idx].rsplit(\".\", 1)[0] + \".txt\"\n","        )\n","        image = Image.open(img_path).convert(\"RGB\")\n","        if self.transform:\n","            image = self.transform(image)\n","\n","        # Create an empty label file if it doesn't exist\n","        if not os.path.exists(label_path):\n","            open(label_path, \"w\").close()\n","\n","        return img_path, label_path\n","\n","\n","# Create custom datasets\n","train_dataset = PlantDataset(\n","    \"data/images/train/\", \"data/labels/train/\", transform=transform\n",")\n","val_dataset = PlantDataset(\"data/images/val/\", \"data/labels/val/\")\n","\n","# Process validation images before training\n","print(\"Processing validation images...\")\n","process_val_images(model)\n","\n","# Train the model with custom settings\n","results = model.train(\n","    data=\"config.yaml\",\n","    epochs=200,\n","    imgsz=640,\n","    batch=4,\n","    optimizer=\"AdamW\",\n","    lr0=1e-3,\n","    lrf=1e-4,\n","    momentum=0.937,\n","    weight_decay=0.0005,\n","    warmup_epochs=3,\n","    warmup_momentum=0.8,\n","    warmup_bias_lr=0.1,\n","    box=7.5,\n","    cls=0.5,\n","    dfl=1.5,\n","    mixup=0.2,\n","    copy_paste=0.1,\n","    device=device,\n","    amp=True,  # Enable mixed precision training\n","    workers=4,  # Number of worker threads for data loading\n","    cos_lr=True,  # Use cosine learning rate scheduler\n","    # resume=True,  # Resume training from last checkpoint\n","    patience=20,  # Early stopping\n","    val=True,\n","    save_period=5,  # Save model every 5 epochs (replaces val_period)\n",")\n","# Evaluate the model\n","val_results = model.val()\n","# Save the best model\n","best_model_path = os.path.join(results.save_dir, \"weights\", \"best.pt\")\n","print(f\"Best model saved as: {best_model_path}\")\n","# Optionally, save the final model explicitly\n","final_model_path = os.path.join(results.save_dir, \"weights\", \"last.pt\")\n","print(f\"Final model saved as: {final_model_path}\")\n","# Run inference on validation images with confidence threshold\n","model.predict(source=\"data/images/val/\", save=True, conf=0.25, device=device)\n","# Export the model to ONNX format for faster inference\n","model.export(format=\"onnx\", dynamic=True, simplify=True)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def main():\n","    # Load pre-trained YOLO model\n","    model = YOLO(\"runs/detect/train/weights/best.pt\")\n","    val_images_path = \"data/images/val\"\n","    val_labels_path = \"data/labels/val\"\n","    # Create val labels directory if it doesn't exist\n","    os.makedirs(val_labels_path, exist_ok=True)\n","\n","    for image_file in os.listdir(val_images_path):\n","        if image_file.lower().endswith((\".png\", \".jpg\", \".jpeg\")):\n","            image_path = os.path.join(val_images_path, image_file)\n","            label_file = os.path.splitext(image_file)[0] + \".txt\"\n","            label_path = os.path.join(val_labels_path, label_file)\n","            new_labels = process_image(image_path, label_path, model)\n","            if new_labels:\n","                # Append new labels to the existing file\n","                with open(label_path, \"a\") as f:\n","                    for label in new_labels:\n","                        f.write(\" \".join(map(str, label)) + \"\\n\")\n","                print(\n","                    f\"Processed {image_file}: {len(new_labels)} new plants detected and labeled\"\n","                )\n","            else:\n","                print(f\"Processed {image_file}: No new plants detected\")\n","\n","\n","main()"]}],"metadata":{"accelerator":"TPU","colab":{"gpuType":"V28","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":0}
